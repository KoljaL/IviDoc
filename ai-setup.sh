#!/bin/bash
# =====================================
# IviDoc AI-Profil Setup
# Aktiviert Ollama LLM + Paperless-AI
# =====================================

set -e

echo "ü§ñ IviDoc AI-Profil Setup"
echo "=========================="
echo ""

# -------------------
# Check Docker
# -------------------
if ! command -v docker &> /dev/null; then
    echo "‚ùå Docker nicht gefunden!"
    echo "   Bitte erst: ./setup.sh --ai"
    exit 1
fi

if ! docker ps &> /dev/null; then
    echo "‚ùå Docker l√§uft nicht!"
    if command -v colima &> /dev/null; then
        echo "üöÄ Starte Colima..."
        colima start --arch aarch64 --cpu 4 --memory 8
    else
        echo "   Bitte Docker Desktop starten."
        exit 1
    fi
fi

echo "‚úÖ Docker l√§uft"
echo ""

# Pr√ºfen ob Basis-System l√§uft
if ! docker compose ps | grep -q "paperless.*running"; then
    echo "‚ùå Paperless l√§uft nicht!"
    echo "   Bitte erst Basis-System starten: docker compose up -d"
    exit 1
fi

echo "‚úì Basis-System l√§uft"
echo ""

# AI-Profil starten
echo "üöÄ Starte AI-Services (Ollama + Paperless-AI)..."
docker compose --profile ai up -d

echo ""
echo "‚è≥ Warte auf Ollama-Start (15 Sek)..."
sleep 15

# Modell-Empfehlungen
echo ""
echo "üìö LLM-Modell installieren:"
echo ""
echo "Empfohlene Modelle f√ºr M4 MacBook Air:"
echo ""
echo "1. Llama 3.2 (3B) - Schnell, Deutsch OK"
echo "   docker compose exec ollama ollama pull llama3.2"
echo "   Gr√∂√üe: ~2.8 GB"
echo ""
echo "2. Mistral (7B) - Besser, etwas langsamer"
echo "   docker compose exec ollama ollama pull mistral"
echo "   Gr√∂√üe: ~4.1 GB"
echo ""
echo "3. Llama 3.1 (8B) - Sehr gut, braucht mehr RAM"
echo "   docker compose exec ollama ollama pull llama3.1:8b"
echo "   Gr√∂√üe: ~4.7 GB"
echo ""
read -p "M√∂chten Sie jetzt Llama 3.2 installieren? (j/n) " -n 1 -r
echo ""

if [[ $REPLY =~ ^[Jj]$ ]]; then
    echo ""
    echo "üì• Lade Llama 3.2 Modell herunter (~2.8 GB)..."
    docker compose exec ollama ollama pull llama3.2
    
    echo ""
    echo "üß™ Teste Modell..."
    docker compose exec ollama ollama run llama3.2 "Hallo! Sage mir in einem Satz, wer du bist."
fi

echo ""
echo "================================================"
echo "‚úÖ AI-Profil ist aktiv!"
echo "================================================"
echo ""
echo "üåê Paperless-NGX: http://localhost:8000"
echo "ü§ñ Ollama API:    http://localhost:11434"
echo ""
echo "üìä Status pr√ºfen:"
echo "   docker compose ps"
echo ""
echo "üí° Modell wechseln:"
echo "   docker compose exec ollama ollama list"
echo "   docker compose exec ollama ollama pull <model>"
echo ""
echo "üõë AI-Services stoppen:"
echo "   docker compose stop ollama paperless-ai"
echo ""
echo "================================================"
