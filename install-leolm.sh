#!/bin/bash
# =====================================
# LeoLM 13B Installation f√ºr IviDoc
# Deutsches LLM optimiert f√ºr Dokumenten-Analyse
# =====================================

set -e

echo "üá©üá™ LeoLM 13B GGUF Installation"
echo "==============================="
echo ""

# Check Docker/Ollama
if ! docker compose ps | grep -q ollama; then
    echo "‚ùå Ollama l√§uft nicht!"
    echo "   Bitte erst starten: docker compose --profile ai up -d"
    exit 1
fi

echo "‚úÖ Ollama l√§uft"
echo ""

# Download-Verzeichnis
MODELS_DIR="$HOME/.ollama/models"
mkdir -p "$MODELS_DIR"

# LeoLM 13B GGUF URL (HuggingFace)
MODEL_URL="https://huggingface.co/TheBloke/leo-hessianai-13B-chat-GGUF/resolve/main/leo-hessianai-13b-chat.Q4_K_M.gguf"
MODEL_FILE="$MODELS_DIR/leo-hessianai-13b-chat.Q4_K_M.gguf"

# Download falls nicht vorhanden
if [ ! -f "$MODEL_FILE" ]; then
    echo "üì• Lade LeoLM 13B GGUF herunter (~7.4 GB)..."
    echo "   Dies kann 5-20 Min dauern..."
    curl -L --progress-bar "$MODEL_URL" -o "$MODEL_FILE"
    echo ""
    echo "‚úÖ Download abgeschlossen"
else
    echo "‚úÖ Modell bereits heruntergeladen"
fi

echo ""

# Modelfile erstellen
MODELFILE_PATH="$MODELS_DIR/Modelfile-leolm"
cat > "$MODELFILE_PATH" << 'EOF'
FROM /root/.ollama/models/leo-hessianai-13b-chat.Q4_K_M.gguf

TEMPLATE """{{ if .System }}<|im_start|>system
{{ .System }}<|im_end|>
{{ end }}{{ if .Prompt }}<|im_start|>user
{{ .Prompt }}<|im_end|>
{{ end }}<|im_start|>assistant
"""

PARAMETER temperature 0.7
PARAMETER top_p 0.9
PARAMETER top_k 40
PARAMETER repeat_penalty 1.1
PARAMETER stop "<|im_start|>"
PARAMETER stop "<|im_end|>"

SYSTEM """Du bist ein hilfreicher deutscher Assistent, spezialisiert auf die Analyse von Dokumenten wie Rechnungen, Vertr√§gen und Briefen. Du extrahierst pr√§zise Informationen und antwortest immer auf Deutsch."""
EOF

echo "üìã Erstelle Modelfile..."

# In Docker-Container kopieren und importieren
echo "üì¶ Kopiere Modell in Ollama-Container..."
docker cp "$MODEL_FILE" ividoc-ollama-1:/root/.ollama/models/
docker cp "$MODELFILE_PATH" ividoc-ollama-1:/root/.ollama/models/Modelfile

echo ""
echo "üîß Importiere in Ollama..."
docker compose exec ollama bash -c "cd /root/.ollama/models && ollama create leolm-german:13b -f Modelfile"

echo ""
echo "‚úÖ LeoLM 13B erfolgreich installiert!"
echo ""

# Test
echo "üß™ Teste Modell..."
echo ""
TEST_PROMPT="Analysiere diesen Text und extrahiere das Datum: 'Rechnung vom 15.03.2024 √ºber 199,99 EUR'"

echo "Prompt: $TEST_PROMPT"
echo ""
echo "Antwort:"
docker compose exec ollama ollama run leolm-german:13b "$TEST_PROMPT"

echo ""
echo "================================================"
echo "‚úÖ Installation abgeschlossen!"
echo "================================================"
echo ""
echo "üìö Verwendung:"
echo "   docker compose exec ollama ollama run leolm-german:13b"
echo ""
echo "üí° F√ºr Paperless-AI:"
echo "   1. Web-UI ‚Üí Settings ‚Üí AI"
echo "   2. Model: leolm-german:13b"
echo "   3. Endpoint: http://ollama:11434"
echo ""
echo "üóëÔ∏è  Modell l√∂schen:"
echo "   docker compose exec ollama ollama rm leolm-german:13b"
echo ""
echo "================================================"

# =====================================
# LeoLM 13B GGUF Installation f√ºr IviDoc
# Deutsches LLM speziell f√ºr Dokumenten-Analyse
# =====================================

set -e

echo "üá©üá™ LeoLM 13B Installation"
echo "=========================="
echo ""

# Check Docker l√§uft
if ! docker compose ps | grep -q ollama; then
    echo "‚ùå Ollama l√§uft nicht!"
    echo "   Starte zuerst: docker compose --profile ai up -d"
    exit 1
fi

echo "‚úÖ Ollama l√§uft"
echo ""

# Download directory
MODELS_DIR="$HOME/.ollama/models"
mkdir -p "$MODELS_DIR"

# LeoLM GGUF herunterladen
echo "üì• Lade LeoLM 13B GGUF herunter (~7.5 GB)..."
echo "   Quelle: HuggingFace - LeoLM/leo-hessianai-13b-chat"
echo ""

cd "$MODELS_DIR"

# Download mit curl (mit Progress)
if [ ! -f "leo-hessianai-13b-chat.Q4_K_M.gguf" ]; then
    curl -L --progress-bar \
        -o leo-hessianai-13b-chat.Q4_K_M.gguf \
        "https://huggingface.co/TheBloke/leo-hessianai-13B-chat-GGUF/resolve/main/leo-hessianai-13b-chat.Q4_K_M.gguf"
    echo ""
    echo "‚úÖ Download abgeschlossen"
else
    echo "‚úÖ GGUF bereits vorhanden"
fi

# Modelfile erstellen
echo ""
echo "üìù Erstelle Modelfile..."

cat > Modelfile << 'EOF'
FROM leo-hessianai-13b-chat.Q4_K_M.gguf

TEMPLATE """{{ if .System }}<|im_start|>system
{{ .System }}<|im_end|>
{{ end }}{{ if .Prompt }}<|im_start|>user
{{ .Prompt }}<|im_end|>
{{ end }}<|im_start|>assistant
"""

PARAMETER stop "<|im_start|>"
PARAMETER stop "<|im_end|>"
PARAMETER temperature 0.7
PARAMETER top_p 0.9
PARAMETER top_k 40

SYSTEM """Du bist ein KI-Assistent, spezialisiert auf die Analyse deutscher Dokumente. Du hilfst bei:
- OCR-Nachbearbeitung und Fehlerkorrektur
- Extraktion von strukturierten Daten (Rechnungsnummern, Daten, Betr√§ge)
- Zusammenfassungen von Dokumenten
- Kategorisierung und Verschlagwortung

Antworte pr√§zise und auf Deutsch."""
EOF

echo "‚úÖ Modelfile erstellt"

# In Docker-Container kopieren
echo ""
echo "üì¶ Importiere Modell in Ollama..."

# Dateien in Container kopieren
docker compose cp "$MODELS_DIR/leo-hessianai-13b-chat.Q4_K_M.gguf" ollama:/tmp/
docker compose cp "$MODELS_DIR/Modelfile" ollama:/tmp/

# Modell erstellen
docker compose exec ollama sh -c "cd /tmp && ollama create leolm:13b -f Modelfile"

echo ""
echo "‚úÖ LeoLM 13B erfolgreich installiert!"

# Aufr√§umen
rm -f "$MODELS_DIR/Modelfile"

# Test
echo ""
echo "üß™ Teste Modell..."
docker compose exec ollama ollama run leolm:13b "Analysiere folgenden Text und extrahiere die Rechnungsnummer: Rechnung Nr. 2024-12345 vom 15.01.2024, Gesamtbetrag: 1.234,56 EUR"

echo ""
echo "================================================"
echo "‚úÖ LeoLM 13B ist bereit!"
echo "================================================"
echo ""
echo "üìä Verwendung:"
echo "   docker compose exec ollama ollama run leolm:13b \"<prompt>\""
echo ""
echo "üîß Paperless-AI Konfiguration:"
echo "   In Web-UI: Settings ‚Üí Paperless-AI"
echo "   Model: leolm:13b"
echo ""
echo "üìù Verf√ºgbare Modelle:"
echo "   docker compose exec ollama ollama list"
echo ""
echo "üíæ Speicherort: $MODELS_DIR"
echo "   Gr√∂√üe: ~7.5 GB"
echo ""
echo "================================================"
